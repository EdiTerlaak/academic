---
title: Mixing up Math (part 1)
author: Edi Terlaak
date: '2021-03-13'
slug: mixing-up-math-part-1
categories:
  - R
tags:
  - education
subtitle: ''
summary: ''
authors: []
lastmod: '2021-03-13T15:59:58+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r load packages and data, include=FALSE}

library(tinytex)
library(tidyverse)
library(readxl)
library(lme4)
library(forcats)
library(patchwork)
```

According to a number of psychologists, we've been teaching math the wrong way. Instead of hammering away on the same topic for weeks, learners of math should mix up, or *interleave*, the concepts they practice. In a 2019 study, Doug Rohrer and colleagues claim to show that interleaved practice can have an effect size of no less than 0,8 (compared to standard *blocked* practice). For educational research this is a spectacular effect size. So let's dive into the details of the study and see if we can replicate the results.  


### Research Design
The study consists of a so called field experiment. That means that the study was not performed under controlled conditions in a university room, but at several schools in Florida. One group of seventh grade kids received interleaved practice questions and the other group received blocked practice material. The questions were the same, so that only the ordering differed between the two groups. After practicing for about three months, the same surprise test was taken by all the kids.

In order to clearly identify what the critical issues with the study design are, we introduce some standard notation on causality. An annoying fact for the purpose of an analysis of causality is that we only ever observe one outcome of an event. If we are dealing with a suspected cause that we call treatment $T_{i}$, then for individual $i$ we may only observe $Y_{i}(1)$ and not $Y_{i}(0)$. That is, for person $i$ we may only observe the effect of $T_{i}=1$ at any one point in time. We could measure the outcome at a later point in time, where we could make sure that $T_{i}=0$, but then not all the conditions will be *exactly* the same. So we can never solve the fundamental problem of what would have happened if things would have been different.

At this point one can either give up, or carry on pragmatically and - as one does so often in statistics in all kinds of guises - take the average of many responses to $T_{i}=1$ and $T_{i}=0$. This give us the average treatment effect that we will here simply write as $Y(1)-Y(0)$. 

There are two more problems though. One is the standard problem of inference to the larger population and the second is that of confounders. These can either be observed or unobserved. We write the observed confounders as $X$ and the unobserved as $U$. The problem with confounders is of course that if the treatment $T_{i}=1$ goes together with $X$ or $U$, we can mistakenly conclude that $T$ is doing the causal work. Confounders can cause error at two stages. The first occurs when a sample is taken and is denoted as $\Delta_{S}$ and the second arises from assigning the treatment and is hence written as $\Delta_{T}$. In both cases there are two types of confounders, so that what we are minimizing is 

$$Estimation\ error=(\Delta_{S_{X}}+\Delta_{S_{U}}) + (\Delta_{T_{X}}+\Delta_{T_{U}})$$.
With regard to the sample part of the error in the estimation of the treatment effect, the authors have made no effort to select a random sample of the US or even Florida population. They persuaded five schools within a 30 min drive from their university to participate in the study and excluded schools that were low performers. There is reason to believe that students in Florida are not that different from at least students in other Western countries with respect to learning math. The effect may also hold only for the four math topics in the experiment that we will discuss shortly and not for others. In both cases it seems reasonable to assume that the results generalize to a wider population because the intervention is at such a fundamental level, but we should realize that they are assumptions.   

The second part of the error concern treatment assignment. Here the authors took care to assign treatments randomly. In this case the unit that was assigned the treatment was the class, for obvious logistical reasons. The randomization assures that $(\Delta_{T_{X}}+\Delta_{T_{U}})$ goes to 0 in the limit. But for every finite sample, confounders could be unevenly distributed between the groups. The researchers therefore used a blocked design, which means that the confounder `Teacher` was evenly divided between the two groups. In practice, only teachers that taught several classes of the same level were included and evenly assigned `blocked` and `interleaved` classes. Read more on the advantages of blocked randomization [here](https://gking.harvard.edu/files/abs/matchse-abs.shtml). 

This means that $\Delta_{T_{X}}$ has been set to exactly 0 by design for $X$ = teacher effects. There can of course still be other confounders $U$. The expected value of the difference in $U$ between the two groups is 0, but not that many units have been assigned randomly. There are 787 students in the study, but only 54 classes have been assigned randomly to the two conditions. We will explore the consequences in part 2, where we turn to inference.


### The data
Let's confirm these numbers by looking at the structure of the data set that is shared publicly [here](https://osf.io/7h9p3/). 
```{r load data, include=FALSE}

rohrer <- read_excel(path="C:/R/Rohrer/OSF_test_scores_rohrer.xlsx")

```

```{r summary, warning=FALSE}
# Classes are distributed per teacher
rohrer %>% group_by(School,Teacher, Class, Group) %>% 
  summarize(n = n()) %>% 
  print(n=10)

```

The table below demonstrates that students are distributed more or less equally among the groups. 

```{r}
# students per Group
rohrer %>% group_by(Group) %>% 
  summarize(n=n())

```

After three months of practicing a surprise test was taken. It contained 16 questions on 4 topics:

*   Writing proportional formulas
*   Algebra with parentheses
*   Solving inequalities
*   Calculations on circles 

We create variables with subscores on these 4 topics on the surprise test, as well as their sum. 

```{r}
# summing individual scores into total and standardize. Also summing partial scores 
rohrer <- rohrer %>% mutate(total = rowSums(.[grep("[A-Z]+[1-4]", names(.))]),
                            stand_total = (total - mean(total))/sd(total),
                            prop_formula = rowSums(.[grep("[G]+[1-4]", names(.))]),
                            inequality = rowSums(.[grep("[I]+[1-4]", names(.))]),
                            parentheses = rowSums(.[grep("[E]+[1-4]", names(.))]),
                            circle = rowSums(.[grep("[C]+[1-4]", names(.))]))

# rename levels of the factor Group (for readability graphs)
rohrer <- rohrer %>% mutate(Group = as.factor(Group),
                            Group = fct_recode(Group,
                                               "blocked" = "block",
                                               "interleaved" = "inter")) 
```

### Visual Exploration

We first inspect the results of students in the two groups per school visually. 

```{r}
# Results total by School and Group in a histogram; alpha can be adjusted to make colors appear; 
# beware though, as they distract
rohrer %>% ggplot(aes(x=total)) +
  geom_histogram(binwidth=1, color="black", fill="grey") +
  facet_grid(rows= vars(Group), cols= vars(School), labeller = label_both) +
  geom_rect(data=subset(rohrer, Group == "blocked"), 
            aes(xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf), 
            fill="red", alpha=0.0015) +
  geom_rect(data=subset(rohrer, Group == "interleaved"), 
            aes(xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=Inf), 
            fill="blue", alpha=0.0015) +
  theme_classic() +
  theme(
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    strip.background = element_rect(
      color="black", fill="white", size=1.5, linetype="solid")) +
  ylab("number of students") +
  xlab("points scored") +
  ggtitle("The effect of interleaving per school")

```

We observe a dramatic difference in performance among the two groups in all schools.

Zooming in, we compare students that were taught by the same teacher, but were either in the blocked or interleaved group.

```{r}
# row and col names have to be assigned manually because automatic ones don't fit
Teach1 <- c("Teacher 1", "Teacher 2","Teacher 3","Teacher 4","Teacher 5")
names(Teach1) <- c("1", "2", "3", "4", "5")
Teach2 <- c("Teacher 6", "Teacher 7","Teacher 8","Teacher 9","Teacher 10")
names(Teach2) <- c("6", "7", "8", "9", "10")
Teach3 <- c("Teacher 11", "Teacher 12","Teacher 13","Teacher 14","Teacher 15")
names(Teach3) <- c("11", "12", "13", "14", "15")
Group1 <- c("block", "inter")
names(Group1) <- c("blocked", "interleaved")

# facet_grid cannot cut up the grid, so we do it ourselves
p1 <- rohrer %>% filter(Teacher <= 5) %>% ggplot(aes(x=total)) +
  geom_histogram(binwidth=1, color="black", fill="grey") +
  scale_y_continuous(breaks=seq(0,10,5)) +
  facet_grid(rows= vars(Group), cols= vars(Teacher), labeller = labeller(Group = Group1, Teacher = Teach1)) +
  theme_classic() +
  theme(
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    strip.background = element_rect(
      color="black", fill="white", size=1.5, linetype="solid")) +
  xlab(NULL) +
  ylab(NULL)

p2 <- rohrer %>% filter(Teacher > 5 & Teacher <=10) %>% ggplot(aes(x=total)) +
  geom_histogram(binwidth=1, color="black", fill="grey") +
  facet_grid(rows= vars(Group), cols= vars(Teacher), labeller = labeller(Group = Group1, Teacher = Teach2)) +
  scale_y_continuous(breaks=seq(0,6,3)) +
  theme_classic() +
  theme(
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    strip.background = element_rect(
      color="black", fill="white", size=1.5, linetype="solid")) +
  ylab("number of students") +
  xlab(NULL)

p3 <- rohrer %>% filter(Teacher > 10) %>% ggplot(aes(x=total)) +
  geom_histogram(binwidth=1, color="black", fill="grey") +
  facet_grid(rows= vars(Group), cols= vars(Teacher), labeller = labeller(Group = Group1, Teacher = Teach3)) +
  scale_y_continuous(breaks=seq(0,10,5)) +
  theme_classic() +
  theme(
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    strip.background = element_rect(
      color="black", fill="white", size=1.5, linetype="solid")) +
  xlab("points scored") +
  ylab(NULL)

# We use patchwork to glue them together in the order that we desire
patch1 <- p1/ p2/ p3

patch1 + plot_annotation(
  title = 'The effect of interleaving per Teacher'
)

```

Here the results are more ambiguous. For some teachers we observe the same stark difference in test results in favor of the interleaved group. But for others the results seem quite similar at first glance. There seems then to be significant variation between teachers in the difference between the groups. This topic will be explored in depth in the analysis in part 2. 

So far we have looked at the total score on the surprise test. We now split up the scores in the four topics that were covered. The order matters here. In the `blocked` condition students first completed questions on formulas for proportional relations, solved inequalities in the second block, etc. We notice a clear pattern. 

```{r}
# Histogram for the four types of questions
h1 <- ggplot(rohrer) +
  geom_histogram(binwidth=1, color="black", fill="grey", aes(y=prop_formula)) +
  coord_flip() +
  facet_grid(vars(Group)) +
  theme_classic()

h2 <- ggplot(rohrer) +
  geom_histogram(binwidth=1, color="black", fill="grey", aes(y=inequality)) +
  coord_flip() +
  facet_grid(vars(Group)) +
  theme_classic()

h3 <- ggplot(rohrer) +
  geom_histogram(binwidth=1, color="black", fill="grey", aes(y=parentheses)) +
  coord_flip() +
  facet_grid(vars(Group)) +
  theme_classic()

h4 <- ggplot(rohrer) +
  geom_histogram(binwidth=1, color="black", fill="grey", aes(y=circle)) +
  coord_flip() +
  facet_grid(vars(Group)) +
  theme_classic()

patch2 <- (h1+h2) / (h3+h4)

patch2 + plot_annotation(
  title = 'The effect of interleaving declines for recently blocked topics',
)

```

The difference in performance seems to decline for topics that were covered more recently in the `blocked` group. This is unsurprising. More interestingly, the `interleaving` group seems to do better even for the questions on circles that were just covered in the blocked group. 

Finally, the last step before we turn to inference is to replicate the finding that Cohen's d is 0.83 for the total score on the test. 

```{r}
# Calculate the pooled standard deviation in Cohen's d
n_blocked <- rohrer %>% filter(Group == "blocked") %>% nrow()
n_interleaved <-  rohrer %>% filter(Group == "interleaved") %>% nrow()
var_blocked <- rohrer %>% filter(Group == "blocked") %>% pull(total) %>% var()
var_interleaved <- rohrer %>% filter(Group == "interleaved") %>% pull(total) %>% var()

pooled_sd <- sqrt(
  ((n_blocked-1)*var_blocked + (n_interleaved - 1)*var_interleaved) /
    (n_blocked + n_interleaved - 2) 
)

# Cohen's d
rohrer %>% summarize(
  d = (mean(total[Group == "interleaved"])- mean(total[Group == "blocked"]))/pooled_sd)
```

This is an impressive effect size. We will check if it holds up in part 2.

